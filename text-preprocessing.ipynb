{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b0c77b8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-02T04:23:52.479505Z",
     "iopub.status.busy": "2025-12-02T04:23:52.479054Z",
     "iopub.status.idle": "2025-12-02T04:23:54.797357Z",
     "shell.execute_reply": "2025-12-02T04:23:54.796234Z"
    },
    "papermill": {
     "duration": 2.331511,
     "end_time": "2025-12-02T04:23:54.799164",
     "exception": false,
     "start_time": "2025-12-02T04:23:52.467653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n",
      "/kaggle/input/twitter-sentiment-analysis-hatred-speech/train.csv\n",
      "/kaggle/input/twitter-sentiment-analysis-hatred-speech/test.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7538a5d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:23:54.819677Z",
     "iopub.status.busy": "2025-12-02T04:23:54.819168Z",
     "iopub.status.idle": "2025-12-02T04:23:56.058541Z",
     "shell.execute_reply": "2025-12-02T04:23:56.057540Z"
    },
    "papermill": {
     "duration": 1.251785,
     "end_time": "2025-12-02T04:23:56.060203",
     "exception": false,
     "start_time": "2025-12-02T04:23:54.808418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca491116",
   "metadata": {
    "papermill": {
     "duration": 0.008643,
     "end_time": "2025-12-02T04:23:56.078413",
     "exception": false,
     "start_time": "2025-12-02T04:23:56.069770",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Different steps for performing text preprocessing in NLP pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a767c",
   "metadata": {
    "papermill": {
     "duration": 0.009164,
     "end_time": "2025-12-02T04:23:56.095938",
     "exception": false,
     "start_time": "2025-12-02T04:23:56.086774",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1) Lowercasing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4996913c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:23:56.117595Z",
     "iopub.status.busy": "2025-12-02T04:23:56.117265Z",
     "iopub.status.idle": "2025-12-02T04:23:56.337308Z",
     "shell.execute_reply": "2025-12-02T04:23:56.336324Z"
    },
    "papermill": {
     "duration": 0.234127,
     "end_time": "2025-12-02T04:23:56.338963",
     "exception": false,
     "start_time": "2025-12-02T04:23:56.104836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>i thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>i am a catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>i'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>no one expects the star trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      one of the other reviewers has mentioned that ...  positive\n",
       "1      a wonderful little production. <br /><br />the...  positive\n",
       "2      i thought this was a wonderful way to spend ti...  positive\n",
       "3      basically there's a family where a little boy ...  negative\n",
       "4      petter mattei's \"love in the time of money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  i thought this movie did a down right good job...  positive\n",
       "49996  bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  i am a catholic taught in parochial elementary...  negative\n",
       "49998  i'm going to have to disagree with the previou...  negative\n",
       "49999  no one expects the star trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df['review'] = df['review'].str.lower()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b0482",
   "metadata": {
    "papermill": {
     "duration": 0.010385,
     "end_time": "2025-12-02T04:23:56.358236",
     "exception": false,
     "start_time": "2025-12-02T04:23:56.347851",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2) Removing HTML tags using regular expressions i.e regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8302320a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:23:56.377287Z",
     "iopub.status.busy": "2025-12-02T04:23:56.376964Z",
     "iopub.status.idle": "2025-12-02T04:23:56.623109Z",
     "shell.execute_reply": "2025-12-02T04:23:56.621829Z"
    },
    "papermill": {
     "duration": 0.258379,
     "end_time": "2025-12-02T04:23:56.625184",
     "exception": false,
     "start_time": "2025-12-02T04:23:56.366805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'',text)\n",
    "\n",
    "df['review'] = df['review'].apply(remove_html_tags)  # apply() is a pandas function used with Series or DataFrame to apply a function to each value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351ecbe0",
   "metadata": {
    "papermill": {
     "duration": 0.00852,
     "end_time": "2025-12-02T04:23:56.642637",
     "exception": false,
     "start_time": "2025-12-02T04:23:56.634117",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3) Removing URLs using regex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f4d8905",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:23:56.662899Z",
     "iopub.status.busy": "2025-12-02T04:23:56.662544Z",
     "iopub.status.idle": "2025-12-02T04:23:57.464098Z",
     "shell.execute_reply": "2025-12-02T04:23:57.463098Z"
    },
    "papermill": {
     "duration": 0.814464,
     "end_time": "2025-12-02T04:23:57.465929",
     "exception": false,
     "start_time": "2025-12-02T04:23:56.651465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For notebook click  to search check \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def remove_url(text) :\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'',text)\n",
    "\n",
    "\n",
    "textl = 'Check out my notebook https://www.kaggle .com/campusx/notebook8223fc1abb'\n",
    "text2 = 'Check out my notebook http://www.kaggle.com/campusx/notebook8223fc1abb'\n",
    "text3 ='Google search here www.google.com'\n",
    "text4 = 'For notebook click https://www.kagg1e.com/campusx/notebook8223fc1abb to search check www.google.com'\n",
    "\n",
    "print(remove_url(text4))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "df['review'] = df['review'].apply(remove_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b809b4",
   "metadata": {
    "papermill": {
     "duration": 0.008616,
     "end_time": "2025-12-02T04:23:57.483732",
     "exception": false,
     "start_time": "2025-12-02T04:23:57.475116",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4) Removing Punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7978173f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:23:57.505281Z",
     "iopub.status.busy": "2025-12-02T04:23:57.504463Z",
     "iopub.status.idle": "2025-12-02T04:23:57.512819Z",
     "shell.execute_reply": "2025-12-02T04:23:57.511987Z"
    },
    "papermill": {
     "duration": 0.021313,
     "end_time": "2025-12-02T04:23:57.514288",
     "exception": false,
     "start_time": "2025-12-02T04:23:57.492975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' string  with  punctuation '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text = ' string ! with : punctuation ,'\n",
    "\n",
    "import string,time\n",
    "exclude = string.punctuation\n",
    "print(exclude,\"\\n\") # all the punctuations which we want to remove\n",
    "\n",
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text = text.replace(char,'')\n",
    "    return text\n",
    "\n",
    "remove_punc(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd88ac0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:23:57.534951Z",
     "iopub.status.busy": "2025-12-02T04:23:57.534627Z",
     "iopub.status.idle": "2025-12-02T04:23:57.541102Z",
     "shell.execute_reply": "2025-12-02T04:23:57.539901Z"
    },
    "papermill": {
     "duration": 0.019042,
     "end_time": "2025-12-02T04:23:57.543067",
     "exception": false,
     "start_time": "2025-12-02T04:23:57.524025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " string  with  punctuation \n",
      "0.0001633167266845703 time to remove the punctuation from given text \n",
      "\n",
      "8.165836334228516 time to remove the punctuation from 50000 rows\n"
     ]
    }
   ],
   "source": [
    "# time required by python to remove punctuation by using the function we defined\n",
    "\n",
    "start = time.time()\n",
    "print(remove_punc(text))\n",
    "time1 = time.time() - start\n",
    "print(time1 ,\"time to remove the punctuation from given text \\n\")\n",
    "\n",
    "print(time1 * 50000 , \"time to remove the punctuation from 50000 rows\") ## to much time is wasted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c0f8a7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:23:57.564340Z",
     "iopub.status.busy": "2025-12-02T04:23:57.563899Z",
     "iopub.status.idle": "2025-12-02T04:23:57.571827Z",
     "shell.execute_reply": "2025-12-02T04:23:57.570433Z"
    },
    "papermill": {
     "duration": 0.020476,
     "end_time": "2025-12-02T04:23:57.573644",
     "exception": false,
     "start_time": "2025-12-02T04:23:57.553168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " string  with  punctuation \n",
      "0.0001811981201171875 time to remove the punctuation from given text \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Proper method to remove punctuation\n",
    "\n",
    "def remove_punc1(text):\n",
    "    return text.translate(str.maketrans('','',exclude))\n",
    "\n",
    "\n",
    "# comparison between two methods\n",
    "start = time.time()\n",
    "print(remove_punc1(text))\n",
    "time2 = time.time() - start\n",
    "print(time2 ,\"time to remove the punctuation from given text \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1be733ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:23:57.594764Z",
     "iopub.status.busy": "2025-12-02T04:23:57.594371Z",
     "iopub.status.idle": "2025-12-02T04:23:57.914037Z",
     "shell.execute_reply": "2025-12-02T04:23:57.912864Z"
    },
    "papermill": {
     "duration": 0.331961,
     "end_time": "2025-12-02T04:23:57.915779",
     "exception": false,
     "start_time": "2025-12-02T04:23:57.583818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  label                                              tweet\n",
      "0   1      0   @user when a father is dysfunctional and is s...\n",
      "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
      "2   3      0                                bihday your majesty\n",
      "3   4      0  #model   i love u take with u all the time in ...\n",
      "4   5      0             factsguide: society now    #motivation\n",
      "          id  label                                              tweet\n",
      "0          1      0   @user when a father is dysfunctional and is s...\n",
      "1          2      0  @user @user thanks for #lyft credit i can't us...\n",
      "2          3      0                                bihday your majesty\n",
      "3          4      0  #model   i love u take with u all the time in ...\n",
      "4          5      0             factsguide: society now    #motivation\n",
      "...      ...    ...                                                ...\n",
      "31957  31958      0  ate @user isz that youuu?√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞...\n",
      "31958  31959      0    to see nina turner on the airwaves trying to...\n",
      "31959  31960      0  listening to sad songs on a monday morning otw...\n",
      "31960  31961      1  @user #sikh #temple vandalised in in #calgary,...\n",
      "31961  31962      0                   thank you @user for you follow  \n",
      "\n",
      "[31962 rows x 3 columns] \n",
      "\n",
      "\n",
      "          id  label                                              tweet\n",
      "0          1      0   user when a father is dysfunctional and is so...\n",
      "1          2      0  user user thanks for lyft credit i cant use ca...\n",
      "2          3      0                                bihday your majesty\n",
      "3          4      0  model   i love u take with u all the time in u...\n",
      "4          5      0               factsguide society now    motivation\n",
      "...      ...    ...                                                ...\n",
      "31957  31958      0  ate user isz that youuu√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò¬ç√∞¬ü¬ò...\n",
      "31958  31959      0    to see nina turner on the airwaves trying to...\n",
      "31959  31960      0  listening to sad songs on a monday morning otw...\n",
      "31960  31961      1  user sikh temple vandalised in in calgary wso ...\n",
      "31961  31962      0                    thank you user for you follow  \n",
      "\n",
      "[31962 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Removing punctuation in twitter hatred speech dataset\n",
    "\n",
    "df1 = pd.read_csv('/kaggle/input/twitter-sentiment-analysis-hatred-speech/train.csv')\n",
    "print(df1.head())\n",
    "print(df1,'\\n\\n')\n",
    "\n",
    "df1['tweet'] = df1['tweet'].apply(remove_punc1)\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf788d99",
   "metadata": {
    "papermill": {
     "duration": 0.009537,
     "end_time": "2025-12-02T04:23:57.934617",
     "exception": false,
     "start_time": "2025-12-02T04:23:57.925080",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5) Chat word treatment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a5b4a6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:23:57.954491Z",
     "iopub.status.busy": "2025-12-02T04:23:57.954154Z",
     "iopub.status.idle": "2025-12-02T04:23:57.966264Z",
     "shell.execute_reply": "2025-12-02T04:23:57.965283Z"
    },
    "papermill": {
     "duration": 0.024292,
     "end_time": "2025-12-02T04:23:57.967969",
     "exception": false,
     "start_time": "2025-12-02T04:23:57.943677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A3': 'Anytime, Anywhere, Anyplace',\n",
       " 'ADIH': 'Another Day In Hell',\n",
       " 'AFK': 'Away From Keyboard',\n",
       " 'AFAIK': 'As Far As I Know',\n",
       " 'ASAP': 'As Soon As Possible',\n",
       " 'ASL': 'Age, Sex, Location',\n",
       " 'ATK': 'At The Keyboard',\n",
       " 'ATM': 'At The Moment',\n",
       " 'BAE': 'Before Anyone Else',\n",
       " 'BAK': 'Back At Keyboard',\n",
       " 'BBL': 'Be Back Later',\n",
       " 'BBS': 'Be Back Soon',\n",
       " 'BFN': 'Bye For Now',\n",
       " 'B4N': 'Bye For Now',\n",
       " 'BRB': 'Be Right Back',\n",
       " 'BRUH': 'Bro',\n",
       " 'BRT': 'Be Right There',\n",
       " 'BSAAW': 'Big Smile And A Wink',\n",
       " 'BTW': 'By The Way',\n",
       " 'BWL': 'Bursting With Laughter',\n",
       " 'CSL': 'Can‚Äôt Stop Laughing',\n",
       " 'CU': 'See You',\n",
       " 'CUL8R': 'See You Later',\n",
       " 'CYA': 'See You',\n",
       " 'DM': 'Direct Message',\n",
       " 'FAQ': 'Frequently Asked Questions',\n",
       " 'FC': 'Fingers Crossed',\n",
       " 'FIMH': 'Forever In My Heart',\n",
       " 'FOMO': 'Fear Of Missing Out',\n",
       " 'FR': 'For Real',\n",
       " 'FWIW': \"For What It's Worth\",\n",
       " 'FYP': 'For You Page',\n",
       " 'FYI': 'For Your Information',\n",
       " 'G9': 'Genius',\n",
       " 'GAL': 'Get A Life',\n",
       " 'GG': 'Good Game',\n",
       " 'GMTA': 'Great Minds Think Alike',\n",
       " 'GN': 'Good Night',\n",
       " 'GOAT': 'Greatest Of All Time',\n",
       " 'GR8': 'Great!',\n",
       " 'HBD': 'Happy Birthday',\n",
       " 'IC': 'I See',\n",
       " 'ICQ': 'I Seek You',\n",
       " 'IDC': 'I Don‚Äôt Care',\n",
       " 'IDK': \"I Don't Know\",\n",
       " 'IFYP': 'I Feel Your Pain',\n",
       " 'ILU': 'I Love You',\n",
       " 'ILY': 'I Love You',\n",
       " 'IMHO': 'In My Honest/Humble Opinion',\n",
       " 'IMU': 'I Miss You',\n",
       " 'IMO': 'In My Opinion',\n",
       " 'IOW': 'In Other Words',\n",
       " 'IRL': 'In Real Life',\n",
       " 'IYKYK': 'If You Know, You Know',\n",
       " 'JK': 'Just Kidding',\n",
       " 'KISS': 'Keep It Simple, Stupid',\n",
       " 'L': 'Loss',\n",
       " 'L8R': 'Later',\n",
       " 'LDR': 'Long Distance Relationship',\n",
       " 'LMK': 'Let Me Know',\n",
       " 'LMAO': 'Laughing My A** Off',\n",
       " 'LOL': 'Laughing Out Loud',\n",
       " 'LTNS': 'Long Time No See',\n",
       " 'M8': 'Mate',\n",
       " 'MFW': 'My Face When',\n",
       " 'MID': 'Mediocre',\n",
       " 'MRW': 'My Reaction When',\n",
       " 'MTE': 'My Thoughts Exactly',\n",
       " 'NVM': 'Never Mind',\n",
       " 'NRN': 'No Reply Necessary',\n",
       " 'NPC': 'Non-Player Character',\n",
       " 'OIC': 'Oh I See',\n",
       " 'OP': 'Overpowered',\n",
       " 'PITA': 'Pain In The A**',\n",
       " 'POV': 'Point Of View',\n",
       " 'PRT': 'Party',\n",
       " 'PRW': 'Parents Are Watching',\n",
       " 'ROFL': 'Rolling On The Floor Laughing',\n",
       " 'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n",
       " 'ROTFLMAO': 'Rolling On The Floor Laughing My A** Off',\n",
       " 'RN': 'Right Now',\n",
       " 'SK8': 'Skate',\n",
       " 'STATS': 'Your Sex And Age',\n",
       " 'SUS': 'Suspicious',\n",
       " 'TBH': 'To Be Honest',\n",
       " 'TFW': 'That Feeling When',\n",
       " 'THX': 'Thank You',\n",
       " 'TIME': 'Tears In My Eyes',\n",
       " 'TLDR': 'Too Long, Didn‚Äôt Read',\n",
       " 'TNTL': 'Trying Not To Laugh',\n",
       " 'TTFN': 'Ta-Ta For Now!',\n",
       " 'TTYL': 'Talk To You Later',\n",
       " 'U': 'You',\n",
       " 'U2': 'You Too',\n",
       " 'U4E': 'Yours For Ever',\n",
       " 'W': 'Win',\n",
       " 'W8': 'Wait...',\n",
       " 'WB': 'Welcome Back',\n",
       " 'WTF': 'What The F**k',\n",
       " 'WTG': 'Way To Go!',\n",
       " 'WUF': 'Where Are You From?',\n",
       " 'WYD': 'What You Doing?',\n",
       " 'WYWH': 'Wish You Were Here',\n",
       " 'ZZZ': 'Sleeping, Bored, Tired'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first finding the meaning of the chatwords\n",
    "\n",
    "text = \"\"\"\n",
    "A3=Anytime, Anywhere, Anyplace\n",
    "ADIH=Another Day In Hell\n",
    "AFK=Away From Keyboard\n",
    "AFAIK=As Far As I Know\n",
    "ASAP=As Soon As Possible\n",
    "ASL=Age, Sex, Location\n",
    "ATK=At The Keyboard\n",
    "ATM=At The Moment\n",
    "BAE=Before Anyone Else\n",
    "BAK=Back At Keyboard\n",
    "BBL=Be Back Later\n",
    "BBS=Be Back Soon\n",
    "BFN=Bye For Now\n",
    "B4N=Bye For Now\n",
    "BRB=Be Right Back\n",
    "BRUH=Bro\n",
    "BRT=Be Right There\n",
    "BSAAW=Big Smile And A Wink\n",
    "BTW=By The Way\n",
    "BWL=Bursting With Laughter\n",
    "CSL=Can‚Äôt Stop Laughing\n",
    "CU=See You\n",
    "CUL8R=See You Later\n",
    "CYA=See You\n",
    "DM=Direct Message\n",
    "FAQ=Frequently Asked Questions\n",
    "FC=Fingers Crossed\n",
    "FIMH=Forever In My Heart\n",
    "FOMO=Fear Of Missing Out\n",
    "FR=For Real\n",
    "FWIW=For What It's Worth\n",
    "FYP=For You Page\n",
    "FYI=For Your Information\n",
    "G9=Genius\n",
    "GAL=Get A Life\n",
    "GG=Good Game\n",
    "GMTA=Great Minds Think Alike\n",
    "GN=Good Night\n",
    "GOAT=Greatest Of All Time\n",
    "GR8=Great!\n",
    "HBD=Happy Birthday\n",
    "IC=I See\n",
    "ICQ=I Seek You\n",
    "IDC=I Don‚Äôt Care\n",
    "IDK=I Don't Know\n",
    "IFYP=I Feel Your Pain\n",
    "ILU=I Love You\n",
    "ILY=I Love You\n",
    "IMHO=In My Honest/Humble Opinion\n",
    "IMU=I Miss You\n",
    "IMO=In My Opinion\n",
    "IOW=In Other Words\n",
    "IRL=In Real Life\n",
    "IYKYK=If You Know, You Know\n",
    "JK=Just Kidding\n",
    "KISS=Keep It Simple, Stupid\n",
    "L=Loss\n",
    "L8R=Later\n",
    "LDR=Long Distance Relationship\n",
    "LMK=Let Me Know\n",
    "LMAO=Laughing My A** Off\n",
    "LOL=Laughing Out Loud\n",
    "LTNS=Long Time No See\n",
    "M8=Mate\n",
    "MFW=My Face When\n",
    "MID=Mediocre\n",
    "MRW=My Reaction When\n",
    "MTE=My Thoughts Exactly\n",
    "NVM=Never Mind\n",
    "NRN=No Reply Necessary\n",
    "NPC=Non-Player Character\n",
    "OIC=Oh I See\n",
    "OP=Overpowered\n",
    "PITA=Pain In The A**\n",
    "POV=Point Of View\n",
    "PRT=Party\n",
    "PRW=Parents Are Watching\n",
    "ROFL=Rolling On The Floor Laughing\n",
    "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
    "ROTFLMAO=Rolling On The Floor Laughing My A** Off\n",
    "RN=Right Now\n",
    "SK8=Skate\n",
    "STATS=Your Sex And Age\n",
    "SUS=Suspicious\n",
    "TBH=To Be Honest\n",
    "TFW=That Feeling When\n",
    "THX=Thank You\n",
    "TIME=Tears In My Eyes\n",
    "TLDR=Too Long, Didn‚Äôt Read\n",
    "TNTL=Trying Not To Laugh\n",
    "TTFN=Ta-Ta For Now!\n",
    "TTYL=Talk To You Later\n",
    "U=You\n",
    "U2=You Too\n",
    "U4E=Yours For Ever\n",
    "W=Win\n",
    "W8=Wait...\n",
    "WB=Welcome Back\n",
    "WTF=What The F**k\n",
    "WTG=Way To Go!\n",
    "WUF=Where Are You From?\n",
    "WYD=What You Doing?\n",
    "WYWH=Wish You Were Here\n",
    "ZZZ=Sleeping, Bored, Tired\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Converting these chat words into dictionary format\n",
    "\n",
    "chat_words = {\n",
    "    key.strip(): value.strip()\n",
    "    for key, value in (line.split(\"=\", 1) for line in text.strip().split(\"\\n\"))\n",
    "}\n",
    "\n",
    "chat_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1eedd38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:23:57.989125Z",
     "iopub.status.busy": "2025-12-02T04:23:57.988799Z",
     "iopub.status.idle": "2025-12-02T04:23:57.994476Z",
     "shell.execute_reply": "2025-12-02T04:23:57.993409Z"
    },
    "papermill": {
     "duration": 0.018643,
     "end_time": "2025-12-02T04:23:57.996144",
     "exception": false,
     "start_time": "2025-12-02T04:23:57.977501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text=[]\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words:\n",
    "            new_text.append(chat_words[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e319a0ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:23:58.017742Z",
     "iopub.status.busy": "2025-12-02T04:23:58.017364Z",
     "iopub.status.idle": "2025-12-02T04:23:58.023690Z",
     "shell.execute_reply": "2025-12-02T04:23:58.022734Z"
    },
    "papermill": {
     "duration": 0.018645,
     "end_time": "2025-12-02T04:23:58.025192",
     "exception": false,
     "start_time": "2025-12-02T04:23:58.006547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good Night baby'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_conversion( 'GN baby' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d45a1a11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:23:58.045347Z",
     "iopub.status.busy": "2025-12-02T04:23:58.045008Z",
     "iopub.status.idle": "2025-12-02T04:23:58.050846Z",
     "shell.execute_reply": "2025-12-02T04:23:58.049938Z"
    },
    "papermill": {
     "duration": 0.017808,
     "end_time": "2025-12-02T04:23:58.052474",
     "exception": false,
     "start_time": "2025-12-02T04:23:58.034666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For Real i was shocked'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_conversion( 'fr i was shocked' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde1ad4f",
   "metadata": {
    "papermill": {
     "duration": 0.009332,
     "end_time": "2025-12-02T04:23:58.071351",
     "exception": false,
     "start_time": "2025-12-02T04:23:58.062019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6) Spelling Correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c72c2a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:23:58.093305Z",
     "iopub.status.busy": "2025-12-02T04:23:58.092462Z",
     "iopub.status.idle": "2025-12-02T04:24:01.514238Z",
     "shell.execute_reply": "2025-12-02T04:24:01.513152Z"
    },
    "papermill": {
     "duration": 3.434304,
     "end_time": "2025-12-02T04:24:01.516006",
     "exception": false,
     "start_time": "2025-12-02T04:23:58.081702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Welcome to Real. It is very beautiful country'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "incorrect_text = 'Welcomme tou Nepal. It is veary beutiful coantry'\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "textBlb = TextBlob(incorrect_text)\n",
    "textBlb.correct().string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a747d57",
   "metadata": {
    "papermill": {
     "duration": 0.010251,
     "end_time": "2025-12-02T04:24:01.535877",
     "exception": false,
     "start_time": "2025-12-02T04:24:01.525626",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7) Removing stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f6fff99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:01.557825Z",
     "iopub.status.busy": "2025-12-02T04:24:01.557305Z",
     "iopub.status.idle": "2025-12-02T04:24:01.573149Z",
     "shell.execute_reply": "2025-12-02T04:24:01.571477Z"
    },
    "papermill": {
     "duration": 0.029084,
     "end_time": "2025-12-02T04:24:01.575060",
     "exception": false,
     "start_time": "2025-12-02T04:24:01.545976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6649318b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:01.597570Z",
     "iopub.status.busy": "2025-12-02T04:24:01.597208Z",
     "iopub.status.idle": "2025-12-02T04:24:01.602945Z",
     "shell.execute_reply": "2025-12-02T04:24:01.601588Z"
    },
    "papermill": {
     "duration": 0.018708,
     "end_time": "2025-12-02T04:24:01.604710",
     "exception": false,
     "start_time": "2025-12-02T04:24:01.586002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word.lower() not in stop_words:\n",
    "            new_text.append(word)\n",
    "    return \" \".join(new_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56c387dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:01.626726Z",
     "iopub.status.busy": "2025-12-02T04:24:01.626361Z",
     "iopub.status.idle": "2025-12-02T04:24:01.634336Z",
     "shell.execute_reply": "2025-12-02T04:24:01.633462Z"
    },
    "papermill": {
     "duration": 0.021418,
     "end_time": "2025-12-02T04:24:01.636247",
     "exception": false,
     "start_time": "2025-12-02T04:24:01.614829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'quick brown fox jumps lazy dog. Avishek Basyal.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(\"the quick brown fox jumps over the lazy dog. I am Avishek Basyal. My is your\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ac8414d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:01.658286Z",
     "iopub.status.busy": "2025-12-02T04:24:01.657967Z",
     "iopub.status.idle": "2025-12-02T04:24:09.928919Z",
     "shell.execute_reply": "2025-12-02T04:24:09.927852Z"
    },
    "papermill": {
     "duration": 8.284301,
     "end_time": "2025-12-02T04:24:09.930809",
     "exception": false,
     "start_time": "2025-12-02T04:24:01.646508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one reviewers mentioned watching 1 oz episode ...\n",
       "1        wonderful little production. filming technique...\n",
       "2        thought wonderful way spend time hot summer we...\n",
       "3        basically there's family little boy (jake) thi...\n",
       "4        petter mattei's \"love time money\" visually stu...\n",
       "                               ...                        \n",
       "49995    thought movie right good job. creative origina...\n",
       "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    catholic taught parochial elementary schools n...\n",
       "49998    going disagree previous comment side maltin on...\n",
       "49999    one expects star trek movies high art, fans ex...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c6192",
   "metadata": {
    "papermill": {
     "duration": 0.010289,
     "end_time": "2025-12-02T04:24:09.951866",
     "exception": false,
     "start_time": "2025-12-02T04:24:09.941577",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8) Handling emojis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "382f4c3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:09.974206Z",
     "iopub.status.busy": "2025-12-02T04:24:09.973846Z",
     "iopub.status.idle": "2025-12-02T04:24:09.980625Z",
     "shell.execute_reply": "2025-12-02T04:24:09.979413Z"
    },
    "papermill": {
     "duration": 0.020715,
     "end_time": "2025-12-02T04:24:09.982671",
     "exception": false,
     "start_time": "2025-12-02T04:24:09.961956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        u\"\\U000024C2-\\U0001F251\"  # Miscellaneous symbols and pictographs\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f4bb063",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:10.004550Z",
     "iopub.status.busy": "2025-12-02T04:24:10.004184Z",
     "iopub.status.idle": "2025-12-02T04:24:10.014072Z",
     "shell.execute_reply": "2025-12-02T04:24:10.012866Z"
    },
    "papermill": {
     "duration": 0.023175,
     "end_time": "2025-12-02T04:24:10.015876",
     "exception": false,
     "start_time": "2025-12-02T04:24:09.992701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what was that Lmao '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji(\"what was that Lmao üòÇüòÇüòÇ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "439ddeda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:10.037777Z",
     "iopub.status.busy": "2025-12-02T04:24:10.037461Z",
     "iopub.status.idle": "2025-12-02T04:24:10.089605Z",
     "shell.execute_reply": "2025-12-02T04:24:10.088189Z"
    },
    "papermill": {
     "duration": 0.065448,
     "end_time": "2025-12-02T04:24:10.091350",
     "exception": false,
     "start_time": "2025-12-02T04:24:10.025902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python is :two_hearts:\n",
      "PUshpa is :fire:\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "print(emoji.demojize( \"python is üíï\" ) )\n",
    "print(emoji.demojize( \"PUshpa is üî•\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f3f8aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T06:02:49.775312Z",
     "iopub.status.busy": "2025-12-01T06:02:49.774922Z",
     "iopub.status.idle": "2025-12-01T06:02:49.779815Z",
     "shell.execute_reply": "2025-12-01T06:02:49.778890Z",
     "shell.execute_reply.started": "2025-12-01T06:02:49.775277Z"
    },
    "papermill": {
     "duration": 0.010281,
     "end_time": "2025-12-02T04:24:10.112174",
     "exception": false,
     "start_time": "2025-12-02T04:24:10.101893",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 9) Tokenization\n",
    "\n",
    "## Different methods of tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996d02d7",
   "metadata": {
    "papermill": {
     "duration": 0.010128,
     "end_time": "2025-12-02T04:24:10.132266",
     "exception": false,
     "start_time": "2025-12-02T04:24:10.122138",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9.1) Using the python split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f6de543",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:10.155473Z",
     "iopub.status.busy": "2025-12-02T04:24:10.155116Z",
     "iopub.status.idle": "2025-12-02T04:24:10.163332Z",
     "shell.execute_reply": "2025-12-02T04:24:10.162003Z"
    },
    "papermill": {
     "duration": 0.022004,
     "end_time": "2025-12-02T04:24:10.165283",
     "exception": false,
     "start_time": "2025-12-02T04:24:10.143279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'college']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenization\n",
    "\n",
    "sent1 = \"I am going to college\"\n",
    "sent1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12554e8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:10.187967Z",
     "iopub.status.busy": "2025-12-02T04:24:10.187605Z",
     "iopub.status.idle": "2025-12-02T04:24:10.194461Z",
     "shell.execute_reply": "2025-12-02T04:24:10.193148Z"
    },
    "papermill": {
     "duration": 0.020618,
     "end_time": "2025-12-02T04:24:10.196363",
     "exception": false,
     "start_time": "2025-12-02T04:24:10.175745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Heollo everyone', ' I am living in my room', ' My name is Avishek Basyal']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "\n",
    "sent2 = \"Heollo everyone. I am living in my room. My name is Avishek Basyal\"\n",
    "sent2.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2901dcb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:10.220053Z",
     "iopub.status.busy": "2025-12-02T04:24:10.219726Z",
     "iopub.status.idle": "2025-12-02T04:24:10.226677Z",
     "shell.execute_reply": "2025-12-02T04:24:10.225471Z"
    },
    "papermill": {
     "duration": 0.021787,
     "end_time": "2025-12-02T04:24:10.228972",
     "exception": false,
     "start_time": "2025-12-02T04:24:10.207185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'loving', 'Nepal!']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## problem with split function\n",
    "\n",
    "sent3 = \"I am loving Nepal!\" # it also tokenize the exclamation with 'Nepal!' . in future if the word 'Nepal' without exclamation appears , it understand both as differenc\n",
    "\n",
    "sent3.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "965cc53d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:10.253210Z",
     "iopub.status.busy": "2025-12-02T04:24:10.252876Z",
     "iopub.status.idle": "2025-12-02T04:24:10.260591Z",
     "shell.execute_reply": "2025-12-02T04:24:10.259361Z"
    },
    "papermill": {
     "duration": 0.022502,
     "end_time": "2025-12-02T04:24:10.262534",
     "exception": false,
     "start_time": "2025-12-02T04:24:10.240032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where do think I should go? I have 3 day holiday']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent4 = 'Where do think I should go? I have 3 day holiday'\n",
    "sent4.split('.')  # split function can split only one thing . here it can't split question mark as questin mark seprates two sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143f69b0",
   "metadata": {
    "papermill": {
     "duration": 0.01032,
     "end_time": "2025-12-02T04:24:10.283896",
     "exception": false,
     "start_time": "2025-12-02T04:24:10.273576",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9.2) Using Regular function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3091594e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:10.307582Z",
     "iopub.status.busy": "2025-12-02T04:24:10.307221Z",
     "iopub.status.idle": "2025-12-02T04:24:10.314224Z",
     "shell.execute_reply": "2025-12-02T04:24:10.313106Z"
    },
    "papermill": {
     "duration": 0.022387,
     "end_time": "2025-12-02T04:24:10.316742",
     "exception": false,
     "start_time": "2025-12-02T04:24:10.294355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'loving', 'Nepal']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "tokens = re.findall(\"[\\w']+\",sent3)\n",
    "tokens  # here it just removes the punctuation marks from sentence 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf8cc6",
   "metadata": {
    "papermill": {
     "duration": 0.010433,
     "end_time": "2025-12-02T04:24:10.338086",
     "exception": false,
     "start_time": "2025-12-02T04:24:10.327653",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9.3) NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25314e52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:10.362554Z",
     "iopub.status.busy": "2025-12-02T04:24:10.362179Z",
     "iopub.status.idle": "2025-12-02T04:24:10.366895Z",
     "shell.execute_reply": "2025-12-02T04:24:10.365971Z"
    },
    "papermill": {
     "duration": 0.018813,
     "end_time": "2025-12-02T04:24:10.368760",
     "exception": false,
     "start_time": "2025-12-02T04:24:10.349947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize , sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86a80e80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:10.391239Z",
     "iopub.status.busy": "2025-12-02T04:24:10.390839Z",
     "iopub.status.idle": "2025-12-02T04:24:10.430689Z",
     "shell.execute_reply": "2025-12-02T04:24:10.429575Z"
    },
    "papermill": {
     "duration": 0.052779,
     "end_time": "2025-12-02T04:24:10.432222",
     "exception": false,
     "start_time": "2025-12-02T04:24:10.379443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'loving', 'Nepal', '!']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent3) # it also tokenize the exclamation seprately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "805af85b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:10.456897Z",
     "iopub.status.busy": "2025-12-02T04:24:10.456492Z",
     "iopub.status.idle": "2025-12-02T04:24:10.464055Z",
     "shell.execute_reply": "2025-12-02T04:24:10.462832Z"
    },
    "papermill": {
     "duration": 0.02197,
     "end_time": "2025-12-02T04:24:10.465809",
     "exception": false,
     "start_time": "2025-12-02T04:24:10.443839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry?',\n",
       " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry?\n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0cf5d775",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:10.488661Z",
     "iopub.status.busy": "2025-12-02T04:24:10.488336Z",
     "iopub.status.idle": "2025-12-02T04:24:10.492960Z",
     "shell.execute_reply": "2025-12-02T04:24:10.492133Z"
    },
    "papermill": {
     "duration": 0.018066,
     "end_time": "2025-12-02T04:24:10.494424",
     "exception": false,
     "start_time": "2025-12-02T04:24:10.476358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sent5 = 'I have a Ph.D in A.I'\n",
    "sent6 =\"We're here to help! mail us at nks@gmail.com\"\n",
    "sent7 = \"A 5km ride cost $0.50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51e2791c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:10.516981Z",
     "iopub.status.busy": "2025-12-02T04:24:10.516674Z",
     "iopub.status.idle": "2025-12-02T04:24:10.523282Z",
     "shell.execute_reply": "2025-12-02T04:24:10.522119Z"
    },
    "papermill": {
     "duration": 0.020238,
     "end_time": "2025-12-02T04:24:10.525305",
     "exception": false,
     "start_time": "2025-12-02T04:24:10.505067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'have', 'a', 'Ph.D', 'in', 'A.I']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ccbf5e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:10.548537Z",
     "iopub.status.busy": "2025-12-02T04:24:10.548163Z",
     "iopub.status.idle": "2025-12-02T04:24:10.554588Z",
     "shell.execute_reply": "2025-12-02T04:24:10.553670Z"
    },
    "papermill": {
     "duration": 0.019854,
     "end_time": "2025-12-02T04:24:10.556112",
     "exception": false,
     "start_time": "2025-12-02T04:24:10.536258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " \"'re\",\n",
       " 'here',\n",
       " 'to',\n",
       " 'help',\n",
       " '!',\n",
       " 'mail',\n",
       " 'us',\n",
       " 'at',\n",
       " 'nks',\n",
       " '@',\n",
       " 'gmail.com']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent6) # it seprates we're as : 'we' , \"'re'\" and it also seprates the gmail also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "557fbb7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:10.579793Z",
     "iopub.status.busy": "2025-12-02T04:24:10.579418Z",
     "iopub.status.idle": "2025-12-02T04:24:10.586243Z",
     "shell.execute_reply": "2025-12-02T04:24:10.585247Z"
    },
    "papermill": {
     "duration": 0.020433,
     "end_time": "2025-12-02T04:24:10.588081",
     "exception": false,
     "start_time": "2025-12-02T04:24:10.567648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', '5km', 'ride', 'cost', '$', '0.50']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6919ef",
   "metadata": {
    "papermill": {
     "duration": 0.011637,
     "end_time": "2025-12-02T04:24:10.610959",
     "exception": false,
     "start_time": "2025-12-02T04:24:10.599322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9.4) spacy -> Better than other generally , but every methods make mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c55cd553",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:10.634582Z",
     "iopub.status.busy": "2025-12-02T04:24:10.634254Z",
     "iopub.status.idle": "2025-12-02T04:24:22.652084Z",
     "shell.execute_reply": "2025-12-02T04:24:22.651202Z"
    },
    "papermill": {
     "duration": 12.031952,
     "end_time": "2025-12-02T04:24:22.653657",
     "exception": false,
     "start_time": "2025-12-02T04:24:10.621705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4887aa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:22.677002Z",
     "iopub.status.busy": "2025-12-02T04:24:22.676451Z",
     "iopub.status.idle": "2025-12-02T04:24:22.726505Z",
     "shell.execute_reply": "2025-12-02T04:24:22.725199Z"
    },
    "papermill": {
     "duration": 0.063978,
     "end_time": "2025-12-02T04:24:22.728512",
     "exception": false,
     "start_time": "2025-12-02T04:24:22.664534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc1 = nlp(sent5)\n",
    "doc2 = nlp(sent6)\n",
    "doc3 = nlp(sent7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe4fcd96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:22.753753Z",
     "iopub.status.busy": "2025-12-02T04:24:22.753425Z",
     "iopub.status.idle": "2025-12-02T04:24:22.758823Z",
     "shell.execute_reply": "2025-12-02T04:24:22.757591Z"
    },
    "papermill": {
     "duration": 0.020251,
     "end_time": "2025-12-02T04:24:22.760582",
     "exception": false,
     "start_time": "2025-12-02T04:24:22.740331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "have\n",
      "a\n",
      "Ph\n",
      ".\n",
      "D\n",
      "in\n",
      "A.I\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print (token)  # here it seprates Ph.D into two seprate tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5453f28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:22.787704Z",
     "iopub.status.busy": "2025-12-02T04:24:22.786419Z",
     "iopub.status.idle": "2025-12-02T04:24:22.792412Z",
     "shell.execute_reply": "2025-12-02T04:24:22.791146Z"
    },
    "papermill": {
     "duration": 0.021929,
     "end_time": "2025-12-02T04:24:22.794105",
     "exception": false,
     "start_time": "2025-12-02T04:24:22.772176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We\n",
      "'re\n",
      "here\n",
      "to\n",
      "help\n",
      "!\n",
      "mail\n",
      "us\n",
      "at\n",
      "nks@gmail.com\n"
     ]
    }
   ],
   "source": [
    "for token in doc2:\n",
    "    print (token) # it writes email perfectly but make mistake in \"we're\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cfdca840",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:22.819954Z",
     "iopub.status.busy": "2025-12-02T04:24:22.819363Z",
     "iopub.status.idle": "2025-12-02T04:24:22.825396Z",
     "shell.execute_reply": "2025-12-02T04:24:22.824137Z"
    },
    "papermill": {
     "duration": 0.021009,
     "end_time": "2025-12-02T04:24:22.826997",
     "exception": false,
     "start_time": "2025-12-02T04:24:22.805988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "5\n",
      "km\n",
      "ride\n",
      "cost\n",
      "$\n",
      "0.50\n"
     ]
    }
   ],
   "source": [
    "for token in doc3:\n",
    "    print (token)  # works perfectly for this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1636ff",
   "metadata": {
    "papermill": {
     "duration": 0.010829,
     "end_time": "2025-12-02T04:24:22.850714",
     "exception": false,
     "start_time": "2025-12-02T04:24:22.839885",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 10) Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5212e64e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:22.874812Z",
     "iopub.status.busy": "2025-12-02T04:24:22.874487Z",
     "iopub.status.idle": "2025-12-02T04:24:22.879804Z",
     "shell.execute_reply": "2025-12-02T04:24:22.878877Z"
    },
    "papermill": {
     "duration": 0.019927,
     "end_time": "2025-12-02T04:24:22.881357",
     "exception": false,
     "start_time": "2025-12-02T04:24:22.861430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()]) # # Splits the text into words, applies stemming to each word, and joins them back with a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4fa0c5f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:22.906122Z",
     "iopub.status.busy": "2025-12-02T04:24:22.905220Z",
     "iopub.status.idle": "2025-12-02T04:24:22.911842Z",
     "shell.execute_reply": "2025-12-02T04:24:22.910953Z"
    },
    "papermill": {
     "duration": 0.020579,
     "end_time": "2025-12-02T04:24:22.913320",
     "exception": false,
     "start_time": "2025-12-02T04:24:22.892741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"walk walks walking walked\"\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97ebda3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:22.937439Z",
     "iopub.status.busy": "2025-12-02T04:24:22.937071Z",
     "iopub.status.idle": "2025-12-02T04:24:22.943579Z",
     "shell.execute_reply": "2025-12-02T04:24:22.942544Z"
    },
    "papermill": {
     "duration": 0.020117,
     "end_time": "2025-12-02T04:24:22.945053",
     "exception": false,
     "start_time": "2025-12-02T04:24:22.924936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probabl my alltim favorit movi a stori of selfless sacrific and dedic to a nobl caus but it not preachi or bore it just never get olc'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets olc'\n",
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a29c36",
   "metadata": {
    "papermill": {
     "duration": 0.011357,
     "end_time": "2025-12-02T04:24:22.968171",
     "exception": false,
     "start_time": "2025-12-02T04:24:22.956814",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 11) Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "333dfa66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T04:24:22.993140Z",
     "iopub.status.busy": "2025-12-02T04:24:22.992827Z",
     "iopub.status.idle": "2025-12-02T04:24:26.681470Z",
     "shell.execute_reply": "2025-12-02T04:24:26.679898Z"
    },
    "papermill": {
     "duration": 3.703717,
     "end_time": "2025-12-02T04:24:26.683251",
     "exception": false,
     "start_time": "2025-12-02T04:24:22.979534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 Lemma               \n",
      "He                   He                  \n",
      "was                  be                  \n",
      "running              run                 \n",
      "and                  and                 \n",
      "eating               eat                 \n",
      "at                   at                  \n",
      "same                 same                \n",
      "time                 time                \n",
      "He                   He                  \n",
      "has                  have                \n",
      "bad                  bad                 \n",
      "habit                habit               \n",
      "of                   of                  \n",
      "swimming             swim                \n",
      "after                after               \n",
      "playing              play                \n",
      "long                 long                \n",
      "hours                hours               \n",
      "in                   in                  \n",
      "the                  the                 \n",
      "Sun                  Sun                 \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "\n",
    "punctuations = '?:!.,'\n",
    "\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words \n",
    "\n",
    "print('{:<20} {:<20}'.format(\"Word\", \"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print('{:<20} {:<20}'.format(word, wordnet_lemmatizer.lemmatize(word , pos='v'))) # you have to specify part of speech"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 100982,
     "sourceId": 239192,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 134715,
     "sourceId": 320111,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 42.542481,
   "end_time": "2025-12-02T04:24:29.453235",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-02T04:23:46.910754",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
